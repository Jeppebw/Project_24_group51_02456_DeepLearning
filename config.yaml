# General settings
project_name: "ehr_mamba_project"
model_type: "ehr_mamba"  # Options: cehr_bert, cehr_bigbird, ehr_mamba, ehr_mamba2
seed: 42  # Random seed for reproducibility

# Data settings
data:
  data_dir: "/Project_24_group51_02456_DeepLearning/P12data"  # Replace with your data directory
  sequence_file: "sequences.json"  # File containing patient sequences
  id_file: "ids.json"  # File containing patient IDs
  vocab_dir: "/path/to/vocab"  # Replace with your vocabulary directory
  val_size: 0.1  # 10% of the data will be used for validation
  tokenizer_type: "fhir"  # Options: fhir or meds
  padding_side: "right"  # Padding on the right for consistency
  return_attention_mask: true
  max_seq_length: 512  # Maximum length of tokenized sequences

# Model settings
model:
  hidden_size: 768  # Model embedding dimension
  num_layers: 12  # Number of Transformer layers
  num_heads: 12  # Number of attention heads per layer
  dropout: 0.1  # Dropout rate for regularization
  vocab_size: 30522  # Vocabulary size based on tokenizer
  use_decoder: true  # Enable decoder functionality (for ehr_mamba)

# Training settings
train:
  batch_size: 32  # Number of samples per batch
  max_epochs: 10  # Maximum number of epochs for training
  learning_rate: 5e-5  # Learning rate for optimizer
  weight_decay: 0.01  # Weight decay for regularization
  grad_accumulation: 4  # Gradient accumulation steps
  warmup_steps: 1000  # Number of warmup steps
  checkpoint_dir: "/path/to/checkpoints"  # Directory to save checkpoints
  resume_checkpoint: null  # Set path if resuming training from a checkpoint

# Logging settings
logging:
  log_dir: "/Project_24_group51_02456_DeepLearning/log"  # Directory for training logs
  log_every_n_steps: 50  # Frequency of logging
  wandb:
    project: "ehr_mamba"
    entity: "your_wandb_entity"  # Replace with your Weights & Biases workspace name

# Hardware settings
hardware:
  gpus: 2  # Number of GPUs to use
  nodes: 1  # Number of nodes for distributed training
  precision: "16-mixed"  # Mixed precision for faster training
  num_workers: 4  # Number of workers for data loading
  pin_memory: true  # Pin memory for DataLoader
  persistent_workers: true  # Keep workers alive between epochs